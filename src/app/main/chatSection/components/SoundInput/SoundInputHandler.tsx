"use client";
import { devLog } from '@/app/_common/utils/logger';
import React, { useState, useRef, useEffect, useCallback } from 'react';
import { FiMic, FiSquare, FiLoader } from 'react-icons/fi';
import { transcribeAudio } from '@/app/_common/api/sttAPI';

interface SoundInputHandlerProps {
    onTranscriptionReady?: (transcription: string) => void;
    className?: string;
    disabled?: boolean;
}

type RecordingState = "idle" | "recording" | "processing";

const SoundInputHandler: React.FC<SoundInputHandlerProps> = ({
    onTranscriptionReady,
    className = '',
    disabled = false
}) => {
    const [state, setState] = useState<RecordingState>("idle");
    const [audioLevel, setAudioLevel] = useState<number>(0);

    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const analyserRef = useRef<AnalyserNode | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const animationRef = useRef<number | null>(null);
    const [mimeType, setMimeType] = useState<string>('');

    // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹…ì„ ìœ„í•œ refë“¤
    const recordingStartTimeRef = useRef<number | null>(null);
    const audioLevelsRef = useRef<number[]>([]);
    const loggingIntervalRef = useRef<number | null>(null);

    // ìë™ ì •ì§€ë¥¼ ìœ„í•œ refë“¤
    const lowLevelStartTimeRef = useRef<number | null>(null);
    const autoStopTimeoutRef = useRef<number | null>(null);

    const playStartSound = useCallback(() => {
        const audioContext = new AudioContext();
        const oscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();

        oscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);

        oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
        oscillator.frequency.setValueAtTime(600, audioContext.currentTime + 0.1);

        gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
        gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.2);

        oscillator.start(audioContext.currentTime);
        oscillator.stop(audioContext.currentTime + 0.2);
    }, []);

    const playEndSound = useCallback(() => {
        const audioContext = new AudioContext();
        const oscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();

        oscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);

        oscillator.frequency.setValueAtTime(600, audioContext.currentTime);
        oscillator.frequency.setValueAtTime(400, audioContext.currentTime + 0.15);
        oscillator.frequency.setValueAtTime(300, audioContext.currentTime + 0.3);

        gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
        gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.35);

        oscillator.start(audioContext.currentTime);
        oscillator.stop(audioContext.currentTime + 0.35);
    }, []);

    // ë…¹ìŒ ì •ì§€
    const stopRecording = useCallback(() => {
        devLog.log('â¹ï¸ Stopping recording...');
        devLog.log(`ğŸ“Š Current state: ${state}, MediaRecorder exists: ${!!mediaRecorderRef.current}`);

        if (mediaRecorderRef.current && state === "recording") {
            mediaRecorderRef.current.stop();
            playEndSound();
            devLog.log('ğŸ”´ Recording stopped, end sound played');
        } else {
            devLog.log('âš ï¸ Recording not stopped - conditions not met');
            if (!mediaRecorderRef.current) devLog.log('âŒ MediaRecorder is null');
            if (state !== "recording") devLog.log(`âŒ State is not recording: ${state}`);

            // ìë™ ì •ì§€ì˜ ê²½ìš°ì—ë„ ì†Œë¦¬ë¥¼ ì¬ìƒí•˜ë„ë¡ ìˆ˜ì •
            if (mediaRecorderRef.current) {
                mediaRecorderRef.current.stop();
                playEndSound();
                devLog.log('ğŸ”´ MediaRecorder stopped, end sound played (auto-stop)');
            }
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
            devLog.log('ğŸ¤ Microphone stream closed');
        }

        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        if (animationRef.current) {
            cancelAnimationFrame(animationRef.current);
            animationRef.current = null;
        }

        // ë¡œê¹… ì¸í„°ë²Œ ì •ë¦¬
        if (loggingIntervalRef.current) {
            clearInterval(loggingIntervalRef.current);
            loggingIntervalRef.current = null;
        }

        // ìë™ ì •ì§€ íƒ€ì´ë¨¸ ì •ë¦¬
        if (autoStopTimeoutRef.current) {
            clearTimeout(autoStopTimeoutRef.current);
            autoStopTimeoutRef.current = null;
        }

        // ë…¹ìŒ ê´€ë ¨ ë°ì´í„° ì´ˆê¸°í™”
        recordingStartTimeRef.current = null;
        audioLevelsRef.current = [];
        lowLevelStartTimeRef.current = null;

        setAudioLevel(0);
    }, [state, playEndSound]);

    const measureAudioLevel = useCallback(() => {
        if (!analyserRef.current) {
            return;
        }

        const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
        analyserRef.current.getByteFrequencyData(dataArray);

        const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
        const normalizedLevel = average / 255;
        setAudioLevel(normalizedLevel);

        // ì˜¤ë””ì˜¤ ë ˆë²¨ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        audioLevelsRef.current.push(normalizedLevel);

        // ë‹¤ìŒ í”„ë ˆì„ì„ ì˜ˆì•½
        animationRef.current = requestAnimationFrame(measureAudioLevel);
    }, []); // stateì™€ silenceTimer ì œê±°

    // ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹… í•¨ìˆ˜
    const logAudioLevels = useCallback(() => {
        if (!recordingStartTimeRef.current) {
            // devLog.log('âŒ recordingStartTimeRef is null');
            return;
        }

        const currentTime = Date.now();
        const elapsedTime = (currentTime - recordingStartTimeRef.current) / 1000;
        // devLog.log(`â° Elapsed time: ${elapsedTime.toFixed(2)}s`);

        if (elapsedTime >= 1) {
            const levels = audioLevelsRef.current;
            const currentLevel = levels.length > 0 ? levels[levels.length - 1] : 0;
            // devLog.log(`ğŸ“Š Current level: ${currentLevel.toFixed(3)}, History length: ${levels.length}`);

            if (levels.length > 0) {
                const averageLevel = levels.reduce((sum, level) => sum + level, 0) / levels.length;
                const levelRatio = averageLevel > 0 ? (currentLevel / averageLevel) : 0;

                // devLog.log(`ğŸµ Audio Level - Current: ${currentLevel.toFixed(3)}, Average: ${averageLevel.toFixed(3)}, Ratio: ${levelRatio.toFixed(3)}x (${(levelRatio * 100).toFixed(1)}%)`);

                // ìë™ ì •ì§€ ë¡œì§
                checkAutoStop(levelRatio);
            } else {
                devLog.log('No audio levels in history');
            }
        } else {
            devLog.log(`â³ Waiting... ${(1.5 - elapsedTime).toFixed(2)}s remaining`);
        }
    }, []); // audioLevel ì˜ì¡´ì„± ì œê±°

    // ìë™ ì •ì§€ ì²´í¬ í•¨ìˆ˜
    const checkAutoStop = useCallback((levelRatio: number) => {
        const currentTime = Date.now();

        if (levelRatio <= 0.9) {
            if (lowLevelStartTimeRef.current === null) {
                // ì²˜ìŒìœ¼ë¡œ 70% ì´í•˜ê°€ ëœ ì‹œì  ê¸°ë¡
                lowLevelStartTimeRef.current = currentTime;
                // devLog.log(`ğŸ”» Audio level dropped below 70% (${(levelRatio * 100).toFixed(1)}%), starting 2s timer...`);

                // 2ì´ˆ í›„ ìë™ ì •ì§€ íƒ€ì´ë¨¸ ì„¤ì •
                autoStopTimeoutRef.current = window.setTimeout(() => {
                    // devLog.log('â° 2 seconds of low audio level detected, auto-stopping recording...');
                    stopRecording();
                }, 1500);
            } else {
                // ì´ë¯¸ 70% ì´í•˜ ìƒíƒœê°€ ì§€ì† ì¤‘
                const lowLevelDuration = (currentTime - lowLevelStartTimeRef.current) / 1000;
                // devLog.log(`ğŸ”» Low level continues for ${lowLevelDuration.toFixed(1)}s (${(levelRatio * 100).toFixed(1)}%)`);
            }
        } else {
            // ë ˆë²¨ì´ 70% ì´ìƒìœ¼ë¡œ íšŒë³µëœ ê²½ìš°
            if (lowLevelStartTimeRef.current !== null) {
                // devLog.log(`ğŸ”º Audio level recovered above 70% (${(levelRatio * 100).toFixed(1)}%), resetting timer`);

                // íƒ€ì´ë¨¸ ë¦¬ì…‹
                lowLevelStartTimeRef.current = null;
                if (autoStopTimeoutRef.current) {
                    clearTimeout(autoStopTimeoutRef.current);
                    autoStopTimeoutRef.current = null;
                }
            }
        }
    }, [stopRecording]);


    const startRecording = useCallback(async () => {
        try {
            devLog.log('ğŸ™ï¸ Starting recording...');
            setState("recording");

            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });

            streamRef.current = stream;
            devLog.log('âœ… Microphone access granted');

            audioContextRef.current = new AudioContext();
            const source = audioContextRef.current.createMediaStreamSource(stream);
            analyserRef.current = audioContextRef.current.createAnalyser();
            analyserRef.current.fftSize = 256;
            source.connect(analyserRef.current);
            devLog.log('ğŸ”Š Audio context and analyser set up');

            const options: MediaRecorderOptions = {};
            if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
                options.mimeType = 'audio/webm;codecs=opus';
                setMimeType('audio/webm');
            } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
                options.mimeType = 'audio/mp4';
                setMimeType('audio/mp4');
            } else {
                setMimeType('audio/webm');
            }

            mediaRecorderRef.current = new MediaRecorder(stream, options);
            chunksRef.current = [];

            mediaRecorderRef.current.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            mediaRecorderRef.current.onstop = () => {
                handleRecordingComplete();
            };

            mediaRecorderRef.current.start();
            devLog.log('ğŸ”´ Recording started');

            // ë…¹ìŒ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            recordingStartTimeRef.current = Date.now();
            audioLevelsRef.current = []; // ì˜¤ë””ì˜¤ ë ˆë²¨ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
            lowLevelStartTimeRef.current = null; // ìë™ ì •ì§€ ê´€ë ¨ ì´ˆê¸°í™”

            devLog.log('ğŸ• Recording start time set, starting logging in 1.5s...');

            // 1.5ì´ˆ í›„ë¶€í„° 100msë§ˆë‹¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ë¡œê¹… ì‹œì‘
            setTimeout(() => {
                devLog.log('â° 1.5s elapsed, starting audio level logging...');
                loggingIntervalRef.current = window.setInterval(() => {
                    devLog.log('ğŸ”„ Logging interval triggered');
                    logAudioLevels();
                }, 100);
            }, 1500);

            playStartSound();
            measureAudioLevel();

        } catch (err) {
            devLog.error('âŒ ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', err);
            setState("idle");
        }
    }, [playStartSound, measureAudioLevel, logAudioLevels, checkAutoStop, state]);
    // ë…¹ìŒ ì™„ë£Œ ì²˜ë¦¬

    const handleRecordingComplete = useCallback(async () => {
        setState("processing");

        try {
            const blob = new Blob(chunksRef.current, { type: mimeType });

            // ì˜¤ë””ì˜¤ í˜•ì‹ ê²°ì •
            const audioFormat = mimeType.includes('webm') ? 'webm' :
                               mimeType.includes('mp4') ? 'mp4' : 'webm';

            // File ê°ì²´ ìƒì„±
            const audioFile = new File([blob], `recording.${audioFormat}`, {
                type: mimeType
            });

            // STT API í˜¸ì¶œ
            const response = await transcribeAudio(audioFile, audioFormat);

            if (response && (response as any).transcription) {
                // ë³€í™˜ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒìœ„ ì»´í¬ë„ŒíŠ¸ë¡œ ì „ë‹¬
                if (onTranscriptionReady) {
                    onTranscriptionReady((response as any).transcription);
                }
            }
        } catch (err) {
            devLog.error('STT ë³€í™˜ ì‹¤íŒ¨:', err);
        } finally {
            setState("idle");
        }
    }, [mimeType, onTranscriptionReady]);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach(track => track.stop());
            }
            if (audioContextRef.current) {
                audioContextRef.current.close();
            }
            if (animationRef.current) {
                cancelAnimationFrame(animationRef.current);
            }
            if (loggingIntervalRef.current) {
                clearInterval(loggingIntervalRef.current);
            }
            if (autoStopTimeoutRef.current) {
                clearTimeout(autoStopTimeoutRef.current);
            }
        };
    }, []);

    const handleClick = () => {
        if (disabled) return;

        if (state === "idle") {
            startRecording();
        } else if (state === "recording") {
            stopRecording();
        }
    };

    return (
        <button
            onClick={handleClick}
            disabled={state === "processing" || disabled}
            className={className}
            style={{
                position: 'relative',
                background: disabled ? '#f9fafb' : (state === "recording" ? '#ef4444' : '#f3f4f6'),
                color: disabled ? '#d1d5db' : (state === "recording" ? 'white' : '#6b7280'),
                borderWidth: '2px',
                borderStyle: 'solid',
                borderColor: disabled ? '#e5e7eb' : (state === "recording" ? '#ef4444' : '#e5e7eb'),
                borderRadius: '1rem',
                padding: '0.75rem',
                cursor: (state === "processing" || disabled) ? 'not-allowed' : 'pointer',
                transition: 'all 0.2s ease',
                display: 'flex',
                alignItems: 'center',
                justifyContent: 'center',
                width: '48px',
                height: '48px',
                opacity: (state === "processing" || disabled) ? 0.6 : 1,
                transform: (state === "processing" || disabled) ? 'none' : undefined
            }}
            onMouseEnter={(e) => {
                if (!disabled && state !== "processing" && state !== "recording") {
                    e.currentTarget.style.background = '#e5e7eb';
                    e.currentTarget.style.color = '#374151';
                    e.currentTarget.style.transform = 'translateY(-1px)';
                    e.currentTarget.style.boxShadow = '0 4px 12px rgba(156, 163, 175, 0.2)';
                }
            }}
            onMouseLeave={(e) => {
                if (!disabled && state !== "processing" && state !== "recording") {
                    e.currentTarget.style.background = '#f3f4f6';
                    e.currentTarget.style.color = '#6b7280';
                    e.currentTarget.style.transform = 'translateY(0)';
                    e.currentTarget.style.boxShadow = 'none';
                }
            }}
            onMouseDown={(e) => {
                if (!disabled && state !== "processing") {
                    e.currentTarget.style.transform = 'translateY(0)';
                }
            }}
        >
            {state === "idle" ? (
                <FiMic style={{ fontSize: '1.1rem', transition: 'transform 0.2s ease' }} />
            ) : state === "recording" ? (
                <FiSquare style={{ fontSize: '1.1rem', transition: 'transform 0.2s ease' }} />
            ) : (
                <FiLoader
                    style={{
                        fontSize: '1.1rem',
                        transition: 'transform 0.2s ease',
                        animation: 'spin 1s linear infinite'
                    }}
                />
            )}
            {state === "recording" && (
                <div
                    style={{
                        position: 'absolute',
                        bottom: '0px',
                        left: '0',
                        right: '0',
                        height: '2px',
                        backgroundColor: 'rgba(255, 255, 255, 0.8)',
                        opacity: Math.max(0.3, audioLevel * 2),
                        transition: 'opacity 0.1s ease',
                        borderRadius: '0 0 0.875rem 0.875rem'
                    }}
                />
            )}
            <style jsx>{`
                @keyframes spin {
                    from {
                        transform: rotate(0deg);
                    }
                    to {
                        transform: rotate(360deg);
                    }
                }
            `}</style>
        </button>
    );
};

export default SoundInputHandler;
export type { SoundInputHandlerProps };
